{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subtaska.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eENmFkCS9QL1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import csv\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zThDWGMQ9GGr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x = [sent len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "\n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k3howllBOr-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def case_normalization(msg):\n",
        "    return msg.strip().lower()\n",
        "\n",
        "def remove_delimiters(msg):\n",
        "    return filter(None, re.split('[ ,.!]', msg))\n",
        "\n",
        "def pre_process_msg(msg):\n",
        "    return remove_delimiters(case_normalization(msg))\n",
        "\n",
        "def parse_data(input_path):\n",
        "    with open(input_path, 'r') as fd:\n",
        "        rd = csv.reader(fd, delimiter='\\t', quotechar='\"')\n",
        "        fields = rd.next()\n",
        "        data = {}\n",
        "        for field in fields:\n",
        "            data[field] = []\n",
        "        for row in rd:\n",
        "            for i, token in enumerate(row):\n",
        "                data[fields[i]].append(token)\n",
        "        return data\n",
        "\n",
        "def get_word2idx(tokenized_corpus):\n",
        "    vocabulary = []\n",
        "    for sentence in tokenized_corpus:\n",
        "        for token in sentence:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary.append(token)\n",
        "    word2idx = {w : idx + 1 for (idx, w) in enumerate(vocabulary)}\n",
        "    word2idx['<pad>'] = 0\n",
        "    return word2idx\n",
        "\n",
        "def get_model_inputs(tokenized_corpus, word2idx, max_len, labels = []):\n",
        "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "    # Create a tensor of a fixed size filled with zeroes for padding\n",
        "    sent_tensor = Variable(torch.zeros((len(vectorized_sents), max_len))).long()\n",
        "    sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "    # Fill it with vectorized sentences\n",
        "    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "        sent_tensor[idx, :sentlen] = LongTensor(sent)\n",
        "    label_tensor = FloatTensor(labels)\n",
        "    return sent_tensor, label_tensor\n",
        "\n",
        "def predict_value(input_tensor, num_of_classes):\n",
        "    prediction = torch.sigmoid(model(input_tensor)).item()\n",
        "    return num_of_classes - 1 if prediction == 1 else int(prediction * num_of_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwLUgXTgO6Qh",
        "colab_type": "code",
        "outputId": "408e461e-15c5-4c13-c8b1-e891f995d1bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5164
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Parse the csv files into appropriate datasets\n",
        "    model_dataset = parse_data('offenseval-training-v1.tsv')\n",
        "    testing_dataset = parse_data('testset-taska.tsv')\n",
        "\n",
        "    model_corpus = map(pre_process_msg, model_dataset['tweet'])\n",
        "    labels = model_dataset['subtask_a']\n",
        "    testing_corpus = map(pre_process_msg, testing_dataset['tweet'])\n",
        "\n",
        "    # Filter out data that is not relevant to the subtask\n",
        "    filtered_corpus = []\n",
        "    filtered_labels = []\n",
        "    for i, (tweet, label) in enumerate(zip(model_corpus, labels)):\n",
        "        if label != \"NULL\":\n",
        "            filtered_corpus.append(tweet)\n",
        "            filtered_labels.append(label)\n",
        "    model_corpus = filtered_corpus\n",
        "    labels = filtered_labels\n",
        "\n",
        "    # Generate classes for each label\n",
        "    class2label = []\n",
        "    label2class = {}\n",
        "    for label in labels:\n",
        "        if label not in label2class:\n",
        "            label2class[label] = len(class2label)\n",
        "            class2label.append(label)\n",
        "    classes = map(lambda label : label2class[label], labels)\n",
        "\n",
        "    # Fix seeds for consistent results\n",
        "    SEED = 234\n",
        "    torch.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # Split the model dataset into training data and validation data\n",
        "    model_size = len(model_corpus)\n",
        "    validation_size = model_size // 10\n",
        "    start = random.randint(0, model_size)\n",
        "    end = start + validation_size\n",
        "    excess = max(end - model_size, 0)\n",
        "    training_corpus = model_corpus[:start] + model_corpus[end - model_size:]\n",
        "    training_target = classes[:start] + classes[end - model_size:]\n",
        "    validation_corpus = model_corpus[start : end] + model_corpus[:excess]\n",
        "    validation_target = classes[start : end] + classes[:excess]\n",
        "\n",
        "    # Other pre-processing helper variables\n",
        "    USE_CUDA = torch.cuda.is_available()\n",
        "    FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
        "    ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n",
        "\n",
        "    # Get word2idx and intialise tensors\n",
        "    word2idx = get_word2idx(training_corpus)\n",
        "    max_len = np.max(np.array([len(sent) for sent in training_corpus]))\n",
        "    training_data_tensor, training_label_tensor = get_model_inputs(training_corpus, word2idx, max_len, training_target)\n",
        "    validation_data_tensor, _ = get_model_inputs(validation_corpus, word2idx, max_len)\n",
        "    testing_data_tensor, _ = get_model_inputs(testing_corpus, word2idx, max_len)\n",
        "\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_SIZE = 100\n",
        "    HIDDEN_LAYER_SIZE = 100\n",
        "    LR = 0.01\n",
        "    EPOCH = 10\n",
        "    OUTPUT_DIM = 1\n",
        "    N_LAYERS = 2\n",
        "    BIDIRECTIONAL = True\n",
        "    DROPOUT = 0.5\n",
        "    BATCH_SIZE = 100\n",
        "\n",
        "    # Initialise RNN model\n",
        "    model = RNN(len(word2idx), EMBEDDING_SIZE, HIDDEN_LAYER_SIZE, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
        "    device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
        "    model = model.to(device)\n",
        "    \n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(1, EPOCH + 1):\n",
        "        print(\"On epoch \" + str(epoch) + \" out of \" + str(EPOCH))\n",
        "        total_loss = 0\n",
        "        losses = []\n",
        "        model.train()\n",
        "        print(\"Starting batch loop...\")\n",
        "        for i in range(0, len(training_data_tensor), BATCH_SIZE):\n",
        "            model.zero_grad()\n",
        "            inputs = training_data_tensor[i : i + BATCH_SIZE].permute(1, 0).to(device)\n",
        "            targets = training_label_tensor[i : i + BATCH_SIZE].unsqueeze(1).to(device)\n",
        "            preds = model(inputs)\n",
        "            loss = loss_function(preds, targets)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "            if i > 0 and i % 500 == 0:\n",
        "                print(\"[%02d/%d] mean_loss : %0.2f, Perplexity : %0.2f\" % (epoch, EPOCH, np.mean(losses), np.exp(np.mean(losses))))\n",
        "                losses = []\n",
        "        print(\"Finished batch loop...\")\n",
        "\n",
        "    # Test the model on the validation data\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    print(\"PREDICTING LABELS ON VALIDATION DATA\")\n",
        "    for validation_input in validation_data_tensor:\n",
        "        model.zero_grad()\n",
        "        predicted_class = predict_value(validation_input.unsqueeze(1).to(device), len(class2label))\n",
        "        preds.append(predicted_class)\n",
        "    print(\"COMPLETED VALIDATION DATA LABEL PREDICTION\")\n",
        "\n",
        "    # Print accuracy and macro f1 measure for validation predictions\n",
        "    acc = accuracy_score(validation_target, preds)\n",
        "    f1 = f1_score(validation_target, preds, average = 'macro')\n",
        "    print(\"Accuracy: \" + str(acc))\n",
        "    print(\"Macro F1 Average: \" + str(f1))\n",
        "\n",
        "    # Plot confusion matrix of validation predictions\n",
        "    cm = confusion_matrix(validation_target, preds)\n",
        "    plt.clf()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
        "    tick_marks = np.arange(len(class2label))\n",
        "    plt.xticks(tick_marks, class2label)\n",
        "    plt.yticks(tick_marks, class2label)\n",
        "    plt.title('Offensive or Not Offensive Confusion Matrix - Validation Data')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    for i in range(len(class2label)):\n",
        "        for j in range(len(class2label)):\n",
        "            plt.text(j, i, str(cm[i][j]))\n",
        "    plt.show()\n",
        "\n",
        "    # Predict labels for testing data\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    print(\"PREDICTING TESTING LABELS NOW\")\n",
        "    for testing_input in testing_data_tensor:\n",
        "        model.zero_grad()\n",
        "        predicted_label = class2label[predict_value(testing_input.unsqueeze(1).to(device), len(class2label))]\n",
        "        preds.append(predicted_label)\n",
        "    print(\"FINISHED PREDICTING TESTING LABELS\")\n",
        "\n",
        "    # Write predictions for testing data into output csv file\n",
        "    csv_data = []\n",
        "    id_values = testing_dataset['id']\n",
        "    for i, id_val in enumerate(id_values):\n",
        "        csv_data.append([id_val, preds[i]])\n",
        "    with open('predictions.csv', 'w') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerows(csv_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On epoch 1 out of 10\n",
            "Starting batch loop...\n",
            "[01/10] mean_loss : 0.66, Perplexity : 1.94\n",
            "[01/10] mean_loss : 0.65, Perplexity : 1.92\n",
            "[01/10] mean_loss : 0.62, Perplexity : 1.86\n",
            "[01/10] mean_loss : 0.65, Perplexity : 1.91\n",
            "[01/10] mean_loss : 0.65, Perplexity : 1.92\n",
            "[01/10] mean_loss : 0.66, Perplexity : 1.94\n",
            "[01/10] mean_loss : 0.64, Perplexity : 1.89\n",
            "[01/10] mean_loss : 0.65, Perplexity : 1.92\n",
            "[01/10] mean_loss : 0.61, Perplexity : 1.85\n",
            "[01/10] mean_loss : 0.61, Perplexity : 1.84\n",
            "[01/10] mean_loss : 0.64, Perplexity : 1.89\n",
            "[01/10] mean_loss : 0.63, Perplexity : 1.89\n",
            "[01/10] mean_loss : 0.62, Perplexity : 1.87\n",
            "[01/10] mean_loss : 0.61, Perplexity : 1.85\n",
            "[01/10] mean_loss : 0.62, Perplexity : 1.86\n",
            "[01/10] mean_loss : 0.61, Perplexity : 1.84\n",
            "[01/10] mean_loss : 0.65, Perplexity : 1.92\n",
            "[01/10] mean_loss : 0.62, Perplexity : 1.85\n",
            "[01/10] mean_loss : 0.62, Perplexity : 1.86\n",
            "[01/10] mean_loss : 0.63, Perplexity : 1.87\n",
            "[01/10] mean_loss : 0.63, Perplexity : 1.87\n",
            "[01/10] mean_loss : 0.60, Perplexity : 1.82\n",
            "[01/10] mean_loss : 0.64, Perplexity : 1.89\n",
            "Finished batch loop...\n",
            "On epoch 2 out of 10\n",
            "Starting batch loop...\n",
            "[02/10] mean_loss : 0.63, Perplexity : 1.88\n",
            "[02/10] mean_loss : 0.62, Perplexity : 1.85\n",
            "[02/10] mean_loss : 0.59, Perplexity : 1.80\n",
            "[02/10] mean_loss : 0.57, Perplexity : 1.77\n",
            "[02/10] mean_loss : 0.59, Perplexity : 1.80\n",
            "[02/10] mean_loss : 0.61, Perplexity : 1.84\n",
            "[02/10] mean_loss : 0.59, Perplexity : 1.80\n",
            "[02/10] mean_loss : 0.58, Perplexity : 1.79\n",
            "[02/10] mean_loss : 0.56, Perplexity : 1.75\n",
            "[02/10] mean_loss : 0.54, Perplexity : 1.72\n",
            "[02/10] mean_loss : 0.56, Perplexity : 1.75\n",
            "[02/10] mean_loss : 0.58, Perplexity : 1.79\n",
            "[02/10] mean_loss : 0.54, Perplexity : 1.72\n",
            "[02/10] mean_loss : 0.54, Perplexity : 1.71\n",
            "[02/10] mean_loss : 0.56, Perplexity : 1.75\n",
            "[02/10] mean_loss : 0.55, Perplexity : 1.73\n",
            "[02/10] mean_loss : 0.57, Perplexity : 1.76\n",
            "[02/10] mean_loss : 0.58, Perplexity : 1.78\n",
            "[02/10] mean_loss : 0.52, Perplexity : 1.69\n",
            "[02/10] mean_loss : 0.55, Perplexity : 1.74\n",
            "[02/10] mean_loss : 0.55, Perplexity : 1.73\n",
            "[02/10] mean_loss : 0.50, Perplexity : 1.66\n",
            "[02/10] mean_loss : 0.57, Perplexity : 1.77\n",
            "Finished batch loop...\n",
            "On epoch 3 out of 10\n",
            "Starting batch loop...\n",
            "[03/10] mean_loss : 0.59, Perplexity : 1.80\n",
            "[03/10] mean_loss : 0.52, Perplexity : 1.69\n",
            "[03/10] mean_loss : 0.50, Perplexity : 1.64\n",
            "[03/10] mean_loss : 0.51, Perplexity : 1.67\n",
            "[03/10] mean_loss : 0.51, Perplexity : 1.66\n",
            "[03/10] mean_loss : 0.49, Perplexity : 1.63\n",
            "[03/10] mean_loss : 0.51, Perplexity : 1.66\n",
            "[03/10] mean_loss : 0.49, Perplexity : 1.64\n",
            "[03/10] mean_loss : 0.47, Perplexity : 1.60\n",
            "[03/10] mean_loss : 0.45, Perplexity : 1.57\n",
            "[03/10] mean_loss : 0.47, Perplexity : 1.60\n",
            "[03/10] mean_loss : 0.48, Perplexity : 1.61\n",
            "[03/10] mean_loss : 0.45, Perplexity : 1.57\n",
            "[03/10] mean_loss : 0.48, Perplexity : 1.61\n",
            "[03/10] mean_loss : 0.50, Perplexity : 1.65\n",
            "[03/10] mean_loss : 0.45, Perplexity : 1.57\n",
            "[03/10] mean_loss : 0.50, Perplexity : 1.65\n",
            "[03/10] mean_loss : 0.49, Perplexity : 1.63\n",
            "[03/10] mean_loss : 0.45, Perplexity : 1.57\n",
            "[03/10] mean_loss : 0.45, Perplexity : 1.57\n",
            "[03/10] mean_loss : 0.48, Perplexity : 1.62\n",
            "[03/10] mean_loss : 0.42, Perplexity : 1.53\n",
            "[03/10] mean_loss : 0.51, Perplexity : 1.67\n",
            "Finished batch loop...\n",
            "On epoch 4 out of 10\n",
            "Starting batch loop...\n",
            "[04/10] mean_loss : 0.54, Perplexity : 1.72\n",
            "[04/10] mean_loss : 0.44, Perplexity : 1.56\n",
            "[04/10] mean_loss : 0.43, Perplexity : 1.54\n",
            "[04/10] mean_loss : 0.43, Perplexity : 1.54\n",
            "[04/10] mean_loss : 0.48, Perplexity : 1.61\n",
            "[04/10] mean_loss : 0.46, Perplexity : 1.58\n",
            "[04/10] mean_loss : 0.45, Perplexity : 1.57\n",
            "[04/10] mean_loss : 0.36, Perplexity : 1.44\n",
            "[04/10] mean_loss : 0.42, Perplexity : 1.52\n",
            "[04/10] mean_loss : 0.37, Perplexity : 1.45\n",
            "[04/10] mean_loss : 0.45, Perplexity : 1.56\n",
            "[04/10] mean_loss : 0.42, Perplexity : 1.52\n",
            "[04/10] mean_loss : 0.39, Perplexity : 1.48\n",
            "[04/10] mean_loss : 0.40, Perplexity : 1.49\n",
            "[04/10] mean_loss : 0.42, Perplexity : 1.53\n",
            "[04/10] mean_loss : 0.42, Perplexity : 1.53\n",
            "[04/10] mean_loss : 0.44, Perplexity : 1.55\n",
            "[04/10] mean_loss : 0.41, Perplexity : 1.51\n",
            "[04/10] mean_loss : 0.37, Perplexity : 1.45\n",
            "[04/10] mean_loss : 0.40, Perplexity : 1.49\n",
            "[04/10] mean_loss : 0.42, Perplexity : 1.53\n",
            "[04/10] mean_loss : 0.41, Perplexity : 1.50\n",
            "[04/10] mean_loss : 0.44, Perplexity : 1.56\n",
            "Finished batch loop...\n",
            "On epoch 5 out of 10\n",
            "Starting batch loop...\n",
            "[05/10] mean_loss : 0.49, Perplexity : 1.63\n",
            "[05/10] mean_loss : 0.39, Perplexity : 1.47\n",
            "[05/10] mean_loss : 0.36, Perplexity : 1.43\n",
            "[05/10] mean_loss : 0.39, Perplexity : 1.48\n",
            "[05/10] mean_loss : 0.38, Perplexity : 1.47\n",
            "[05/10] mean_loss : 0.42, Perplexity : 1.52\n",
            "[05/10] mean_loss : 0.40, Perplexity : 1.49\n",
            "[05/10] mean_loss : 0.36, Perplexity : 1.43\n",
            "[05/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[05/10] mean_loss : 0.35, Perplexity : 1.42\n",
            "[05/10] mean_loss : 0.36, Perplexity : 1.43\n",
            "[05/10] mean_loss : 0.40, Perplexity : 1.50\n",
            "[05/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[05/10] mean_loss : 0.39, Perplexity : 1.47\n",
            "[05/10] mean_loss : 0.40, Perplexity : 1.49\n",
            "[05/10] mean_loss : 0.35, Perplexity : 1.42\n",
            "[05/10] mean_loss : 0.41, Perplexity : 1.51\n",
            "[05/10] mean_loss : 0.37, Perplexity : 1.45\n",
            "[05/10] mean_loss : 0.31, Perplexity : 1.37\n",
            "[05/10] mean_loss : 0.37, Perplexity : 1.45\n",
            "[05/10] mean_loss : 0.38, Perplexity : 1.46\n",
            "[05/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[05/10] mean_loss : 0.37, Perplexity : 1.44\n",
            "Finished batch loop...\n",
            "On epoch 6 out of 10\n",
            "Starting batch loop...\n",
            "[06/10] mean_loss : 0.43, Perplexity : 1.54\n",
            "[06/10] mean_loss : 0.36, Perplexity : 1.43\n",
            "[06/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[06/10] mean_loss : 0.37, Perplexity : 1.44\n",
            "[06/10] mean_loss : 0.35, Perplexity : 1.42\n",
            "[06/10] mean_loss : 0.33, Perplexity : 1.39\n",
            "[06/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[06/10] mean_loss : 0.28, Perplexity : 1.33\n",
            "[06/10] mean_loss : 0.32, Perplexity : 1.37\n",
            "[06/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "[06/10] mean_loss : 0.36, Perplexity : 1.43\n",
            "[06/10] mean_loss : 0.35, Perplexity : 1.42\n",
            "[06/10] mean_loss : 0.35, Perplexity : 1.41\n",
            "[06/10] mean_loss : 0.31, Perplexity : 1.37\n",
            "[06/10] mean_loss : 0.36, Perplexity : 1.43\n",
            "[06/10] mean_loss : 0.35, Perplexity : 1.42\n",
            "[06/10] mean_loss : 0.38, Perplexity : 1.46\n",
            "[06/10] mean_loss : 0.35, Perplexity : 1.41\n",
            "[06/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[06/10] mean_loss : 0.31, Perplexity : 1.36\n",
            "[06/10] mean_loss : 0.33, Perplexity : 1.39\n",
            "[06/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[06/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "Finished batch loop...\n",
            "On epoch 7 out of 10\n",
            "Starting batch loop...\n",
            "[07/10] mean_loss : 0.40, Perplexity : 1.49\n",
            "[07/10] mean_loss : 0.33, Perplexity : 1.39\n",
            "[07/10] mean_loss : 0.29, Perplexity : 1.33\n",
            "[07/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "[07/10] mean_loss : 0.35, Perplexity : 1.41\n",
            "[07/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "[07/10] mean_loss : 0.34, Perplexity : 1.41\n",
            "[07/10] mean_loss : 0.30, Perplexity : 1.36\n",
            "[07/10] mean_loss : 0.30, Perplexity : 1.34\n",
            "[07/10] mean_loss : 0.23, Perplexity : 1.25\n",
            "[07/10] mean_loss : 0.27, Perplexity : 1.32\n",
            "[07/10] mean_loss : 0.30, Perplexity : 1.36\n",
            "[07/10] mean_loss : 0.27, Perplexity : 1.32\n",
            "[07/10] mean_loss : 0.29, Perplexity : 1.33\n",
            "[07/10] mean_loss : 0.33, Perplexity : 1.39\n",
            "[07/10] mean_loss : 0.29, Perplexity : 1.33\n",
            "[07/10] mean_loss : 0.31, Perplexity : 1.37\n",
            "[07/10] mean_loss : 0.26, Perplexity : 1.30\n",
            "[07/10] mean_loss : 0.27, Perplexity : 1.30\n",
            "[07/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[07/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "[07/10] mean_loss : 0.33, Perplexity : 1.39\n",
            "[07/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "Finished batch loop...\n",
            "On epoch 8 out of 10\n",
            "Starting batch loop...\n",
            "[08/10] mean_loss : 0.37, Perplexity : 1.44\n",
            "[08/10] mean_loss : 0.29, Perplexity : 1.34\n",
            "[08/10] mean_loss : 0.28, Perplexity : 1.32\n",
            "[08/10] mean_loss : 0.32, Perplexity : 1.38\n",
            "[08/10] mean_loss : 0.31, Perplexity : 1.36\n",
            "[08/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[08/10] mean_loss : 0.31, Perplexity : 1.36\n",
            "[08/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[08/10] mean_loss : 0.24, Perplexity : 1.28\n",
            "[08/10] mean_loss : 0.22, Perplexity : 1.25\n",
            "[08/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[08/10] mean_loss : 0.26, Perplexity : 1.29\n",
            "[08/10] mean_loss : 0.25, Perplexity : 1.29\n",
            "[08/10] mean_loss : 0.26, Perplexity : 1.30\n",
            "[08/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[08/10] mean_loss : 0.25, Perplexity : 1.29\n",
            "[08/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[08/10] mean_loss : 0.28, Perplexity : 1.32\n",
            "[08/10] mean_loss : 0.23, Perplexity : 1.26\n",
            "[08/10] mean_loss : 0.29, Perplexity : 1.34\n",
            "[08/10] mean_loss : 0.33, Perplexity : 1.39\n",
            "[08/10] mean_loss : 0.29, Perplexity : 1.34\n",
            "[08/10] mean_loss : 0.29, Perplexity : 1.34\n",
            "Finished batch loop...\n",
            "On epoch 9 out of 10\n",
            "Starting batch loop...\n",
            "[09/10] mean_loss : 0.35, Perplexity : 1.42\n",
            "[09/10] mean_loss : 0.25, Perplexity : 1.29\n",
            "[09/10] mean_loss : 0.26, Perplexity : 1.29\n",
            "[09/10] mean_loss : 0.28, Perplexity : 1.32\n",
            "[09/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[09/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[09/10] mean_loss : 0.30, Perplexity : 1.36\n",
            "[09/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[09/10] mean_loss : 0.24, Perplexity : 1.27\n",
            "[09/10] mean_loss : 0.19, Perplexity : 1.21\n",
            "[09/10] mean_loss : 0.24, Perplexity : 1.27\n",
            "[09/10] mean_loss : 0.23, Perplexity : 1.26\n",
            "[09/10] mean_loss : 0.22, Perplexity : 1.25\n",
            "[09/10] mean_loss : 0.26, Perplexity : 1.30\n",
            "[09/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[09/10] mean_loss : 0.26, Perplexity : 1.29\n",
            "[09/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[09/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[09/10] mean_loss : 0.21, Perplexity : 1.23\n",
            "[09/10] mean_loss : 0.23, Perplexity : 1.26\n",
            "[09/10] mean_loss : 0.29, Perplexity : 1.34\n",
            "[09/10] mean_loss : 0.34, Perplexity : 1.40\n",
            "[09/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "Finished batch loop...\n",
            "On epoch 10 out of 10\n",
            "Starting batch loop...\n",
            "[10/10] mean_loss : 0.29, Perplexity : 1.33\n",
            "[10/10] mean_loss : 0.20, Perplexity : 1.22\n",
            "[10/10] mean_loss : 0.25, Perplexity : 1.28\n",
            "[10/10] mean_loss : 0.24, Perplexity : 1.27\n",
            "[10/10] mean_loss : 0.32, Perplexity : 1.37\n",
            "[10/10] mean_loss : 0.20, Perplexity : 1.22\n",
            "[10/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[10/10] mean_loss : 0.25, Perplexity : 1.28\n",
            "[10/10] mean_loss : 0.21, Perplexity : 1.23\n",
            "[10/10] mean_loss : 0.18, Perplexity : 1.20\n",
            "[10/10] mean_loss : 0.25, Perplexity : 1.29\n",
            "[10/10] mean_loss : 0.23, Perplexity : 1.26\n",
            "[10/10] mean_loss : 0.24, Perplexity : 1.28\n",
            "[10/10] mean_loss : 0.27, Perplexity : 1.30\n",
            "[10/10] mean_loss : 0.30, Perplexity : 1.35\n",
            "[10/10] mean_loss : 0.25, Perplexity : 1.28\n",
            "[10/10] mean_loss : 0.27, Perplexity : 1.31\n",
            "[10/10] mean_loss : 0.24, Perplexity : 1.27\n",
            "[10/10] mean_loss : 0.19, Perplexity : 1.21\n",
            "[10/10] mean_loss : 0.25, Perplexity : 1.28\n",
            "[10/10] mean_loss : 0.29, Perplexity : 1.33\n",
            "[10/10] mean_loss : 0.24, Perplexity : 1.27\n",
            "[10/10] mean_loss : 0.24, Perplexity : 1.27\n",
            "Finished batch loop...\n",
            "PREDICTING LABELS ON VALIDATION DATA\n",
            "COMPLETED VALIDATION DATA LABEL PREDICTION\n",
            "Accuracy: 0.736404833836858\n",
            "Macro F1 Average: 0.6766924500512519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAFnCAYAAABQCHkiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "PREDICTING TESTING LABELS NOW\n",
            "FINISHED PREDICTING TESTING LABELS\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}